#############################
####  Multicolinealidad  ####
#############################

#########################
#### Seteo y cargas  ####
#########################

#Limpiamos todo lo que haya anterior
{rm(list=ls())
  
#Definimos el path y seteamos#  
setwd("C:/Users/carlo/Dropbox/Archivos de Apuntes/Regresion Lineal/Multicolinealidad")
oldpar<-par(no.readonly = TRUE)
options(scipen=99, digits=6)
  
#####################################
#### Instalación y carga         ####
####  de la librerías necesarias ####
#####################################

#El paquete con las bases de Gujarati se instalan desde github
#devtools::install_github('https://github.com/brunoruas2/gujarati')

devtools::install_github('https://github.com/brunoruas2/gujarati')

#Cargamos las librerías necesarias
library("Hmisc")
library("psych")     #Para pairs.panels
library("ggm")       #Para correlacion parcor
library("corrplot")  #Grafico de correlaciones
library("mctest")    #Paquete para multicolinealidad
library("car")       #Para elipses de Confianza
library("olsrr")     #Para salidas de regresion
library("gujarati")   #Para las bases de Gujarati
}

##############################
#### Levantamos la base   ####
#### y armamos los datos  ####
##############################

#Usamos la base Phillips de Wooldridge Ejemplo  pag 390

data('Table10_7')
datos <- Table10_7
datos <- as.data.frame(lapply(datos, function(x) as.numeric(as.character(x))))

#Las variables a usar son
# C  = Consumo
# Yd = Ingreso disponible
# W  = Riqueza real
# I  = Tasa de Interés

colnames(datos) <- c("Año","Consumo","Ingreso","Riqueza","Interes")
attach(datos)

#################################
####  Análisis Exploratorio  ####
####    de la variables      ####
#################################

#Gráfico de correlaciones
{col3 <- colorRampPalette(c("red", "white", "deepskyblue"))
corrplot(cor(datos[,2:5]),method=c("circle"),type="upper",main="Correlaciones",bg="lightblue",
         tl.pos="d",mar=c(3,3,3,3),col=col3(200),cl.ratio = 0.3)
corrplot(cor(datos[,2:5]), add = TRUE, type = "lower", method = "number",diag = FALSE, tl.pos = "n", cl.pos = "n",
         number.digits=4,mar=c(3,3,3,3),col=col3(200))
}

#Matriz de correlaciones simples
cor(datos[,2:5])

####Matriz de correlaciones parciales####

#Pedimos la matriz de correlaciones parciales basadas en la matriz de covarianzas
#0.4892529 es la correlación entre consumo e ingreso sin tener en cuenta a riqueza
#Esta funcion no permite especificar alguna, da toda la matriz
parcor (cov (datos[,2:5]) )

#Otra forma es usando pcor donde podemos definir de que variables queremos y cuales controlamos
#2:4 indica desde la 2 a la 4, la primera es la Y, la dos es la X, las otras son las controladas
pcor(2:4,cov(datos))

#(2,4,3) relacion entre 2 y 4 controlando la 3
pcor(c(2,4,3),cov(datos))

#Vamos a calcular nosotros este valor entre "Consumo e Riqueza" osea, el primero controlando Ingreso
#Las regresiones que sea armen tendrán como variables dependientes a las dos de las que quiero saber la correlacion parcial
#y como dependientes a las que controle

#Para sacar la influencia, usaremos los residuos corriendo la regresión entre Consumo y Riqueza sacando Ingreso
{reg23<-lm(Consumo ~ Ingreso)
  
#La otra regresión es entre Ingreso y Riqueza sacando "Consumo"
reg43<-lm(Riqueza ~ Ingreso)
  
#Hacemos la regresión entre los residuos
reg24<-lm(reg23$residuals ~ reg43$residuals)
}

#Comparamos los valores hallados
pcor(c(2,4,3),cov(datos))
sqrt(summary(reg24)$r.squared)

##################################
### Regresion con colinealidad ###
###       perfecta             ###
##################################

#Armamos una matrix 2x2 simulando 2 datos de 2 variables independientes donde una es el doble o la mitad que la otra.
{x<-matrix(c(2,4,4,8),nrow = 2,byrow = T);x

#Pedimos la inversa de la matriz y vemos que nos dice
solve(x)
#Vemos que es imposible calcular la inversa por ser una columna combinacion lineal de la otra
}

#Hacemos la regresión lineal con estas dos variables y una variable cualquiera y
{y<-matrix(c(5,3),2)
  summary(lm(y~x[,1]+x[,2]))
  
#Este mensaje en la salida "Coefficients: (1 not defined because of singularities)" denota colinealidad perfecta
}

##################################
####  Regresión de los datos  ####
##################################

#Hacemos la regresión usando los logaritmos de las variables para tener las elasticidades
reg<-lm( log(Consumo) ~ log(Ingreso) + log(Riqueza)  + Interes )
reg<-lm( Consumo ~ Ingreso + Riqueza  + Interes, data = datos[1:35,] )
ols_regress( reg )

#Sacamos Interés porque es la variable con menos correlación con Consumo
reg1<-lm(Consumo ~ Ingreso + Riqueza , data = datos[1:35,])
ols_regress(reg1)

reg1<-lm(Consumo ~ Ingreso + Interes , data = datos[1:35,])
ols_regress(reg1)

###############################
####  Medidas de análisis  ####
####  de Multicolinealidad ####
###############################

#https://journal.r-project.org/archive/2016-2/RJ-2016-2.pdf Pagina 497 mctest

#"type=b" analiza las medidas de cada uno y de el conjunto de variables
mctest(reg,type="b")

#Esta funcion calcula los generales
omcdiag(reg)

imcdiag(reg)

########################################
#### Calculo manual de las medidas  ####
########################################

#Transformamos los datos para poder hacerlo
data <- cbind(log(datos[,3:4]),datos[,5])

####Determinante####
det(cor(data))

#Farrar-Glauber
#Definimos los parametros
p<-ncol(data)
n<-nrow(data)

#Valor de Farrar-Glauber
#Este estadistico se distribuye como una chi con p*(p-1) GL
-(n-1-(2*p+5)/6)*log(det(cor(data)))
#R lo calcula asi #EN TEORIA LO CORRIGIO DESPUES DEL MAIL!!!#######
p<-ncol(familia)-1
-(n - 1 - (1/p) * (2 * p + 5))*log(det(cor(data)))


###Calculo del VIF####
#Calculamos el Vif que es la relacion entre una X y el resto, para eso sacamos el R2 de la regresion auxiliar
#regresion auxiliar es la regresion entre variables independientes
reg.x<-lm(log(Riqueza)~log(Ingreso) + Interes)
summary.x<-summary(reg.x)
vif<-1/(1-summary.x$r.squared);vif

#El VIF puede calcularse directamente con (x¨x)^-1 pero de los datos estandarizados
#y esto hace que no sea necesario el vector de Unos
z <- model.frame(reg)
z1 <- scale(z[,2:4])

#Hacemos la inversa de la matriz de correlaciones de las variables estandarizadas
#Neter LI Kutner pag 408 10.39
diag(solve(cor(z1)))

#O lo sacamos directamente con una función
vif(reg)

#Suma de los recíprocos de los valores propios.
#Calculamos los valores propios de la matriz de correlaciones.
e<-eigen(cor(data))$values
sum(1/e)

#Red Indicator
sqrt(sum((e-1)^2))/(p/sqrt(p-1))

#El paquete de R lo calcula de esta forma
x<-data
sx <- scale(x)/sqrt(n - 1)
nvar<-ncol(x)
Red <- sqrt((sum((t(sx) %*% sx)^2) - nvar)/(nvar * (nvar - 1)));Red

####Indice y numero de condición####
#El primero es el máximo sobre cada uno, el segundo el máximo sobre el mínimo
#Para esto necesitamos armar una matriz, de los datos con la columna de unos al principio
uno<-matrix(rep(1,15));uno

#Armamos la matriz con este "uno" al principio y luego las independientes respetando este orden
x<-cbind(uno,familia$Ingreso,familia$Riqueza);x

#Necesitamos la matriz normalizada de X, es decir la inversa de la raiz de la diagonal de t(x)*X
#Vamos paso a paso, armamos la matriz t(x)*x
h<-t(x)%*%x;h

#La matriz normalizada es tomar la diagonal, hacerle la raiz,armar una matriz diagonal y hacer inversa
#Si se lee la formula de adentro hacia afuera, vemos que hace eso
g<-solve(diag(sqrt(diag(h))));g

#Normalizamos la matriz pre y pos multiplicamos por la matriz anterior
xnorm<-g%*%h%*%g;xnorm

#Teniendo la matriz cuadrada, sacamos los autovalores y los autovectores
lambda<-eigen(xnorm);lambda

#Guardamos los autovalores
lambda2<-lambda$values;lambda2

#Indice de condición: Maximo lambda sobre cada lambda
(IC<-sqrt(max(lambda2)/lambda2))

#Numero de condicion: Es el maximo lambda sobre el mínimo
NC<-sqrt(max(lambda2)/min(lambda2));NC
