#################################
#####  REGRESION LOGISTICA   ####
#################################

#########################
#### Seteo y cargas  ####
#########################

#Seteo y carga de librerías####
{setwd("C:/Users/carlo/Dropbox/Archivos de Apuntes/Regresion Logistica/Bases y Sintaxis")
  
  #Limpiamos todo y seteamos
  rm(list=ls())
  oldpar<-par(no.readonly = TRUE)
  options("scipen"=999, "digits"=4)

###################################
####  Instalación y carga de   ####
####  la librerías necesarias  ####
###################################

#Cargamos las librerías####
  library("readxl")
  library("lmtest")
  library("MASS")
  library("RcmdrMisc")     #Para Stepwise
  library("DescTools")     #Para los pseudoR2
  library("generalhoslem") #Para el Hosmer-Lemeshow
  library("caret")         #Tabla de clasificación
  library("pROC")          #Curva ROC
  library("ROCR")          #Medidas de performance (performance del modelo y datos para la ROC)
  library("scorecard")     #Para WOE e IV
  library ("glmulti")      #Para mejor regresión logística
  }

##############################
#### Levantamos la base   ####
#### y armamos los datos  ####
##############################

#La base a analizar corresponde a la Ennys 2 "Encuesta Nacional de Nutricion y Salud 2".
#Se usa una base reducida enfocada en el analisis de si las personas encuestadas
#leen el rotulado nutricional de los alimentos.

####Levantamos la base completa####
base <- as.data.frame( read_xlsx("Ennys 2 - Base Reducida.xlsx",range = "A3:W21361") )

######################################
#### Transformación de los datos  ####
######################################

#Lo primero que hacemos es quedarnos con los registros de personas de 18 o mas
#Esta variable corresponde a "E_CUEST" = 18 o mas años
base <- base[base$E_CUEST=="18 o mas años",]
attach(base)

#Cambiamos la escala de medición de Ingreso para expresarlo en miles
base$SD_28_imputado <- base$SD_28_imputado / 1000

#Creamos una nueva variable
#Hay dos variables que pueden unirse ya que una analiza tipo de ocupación y la otra 
#tipo de inactividad
table(M01_SD_21, useNA = "always")
table(M01_SD_24, useNA = "always")

#Las unimos
base$Ocupacion <- ifelse(is.na(base$M01_SD_21),base$M01_SD_24,base$M01_SD_21)
table(base$Ocupacion , useNA = "always")

#Creamos una variable buscando detectar si hay menores en el grupo familiar analizando la edad
#de los 5 primeros miembros de la familia si alguno tiene menos de 18 años
base$Menores <- ifelse( (!is.na(base$M02_SD_4) & base$M02_SD_4 <= 18)  | 
                      (!is.na(base$M03_SD_4) & base$M03_SD_4 <= 18)  |
                      (!is.na(base$M04_SD_4) & base$M04_SD_4 <= 18)  |
                      (!is.na(base$M05_SD_4) & base$M05_SD_4 <= 18)   , "Si","No")

table(base$Menores, useNA = "always")
detach(base)

#Creamos un set de solo las variables de interés para comenzar la segunda etapa
datos <- base[,c(3,4,5,6,7,8,14,15,24,25)]

#Sacamos los NA
datos <- na.omit(datos)

#Renombramos las variables para hacerlo mas claro
colnames(datos) <- c("Ingreso","Miembros","Edad","Sexo","Medicina","Educacion","Lee","Ponderacion","Ocupacion","Menores")

#Pasando a factor la variable target "Lee", los "NS/Nc" se ponen como "No"
table(datos$Lee,useNA = "always")
datos$Lee <- ifelse(datos$Lee=="Ns/Nc","No",datos$Lee)
table(datos$Lee,useNA = "always")
datos$Lee <- factor(datos$Lee, levels = c("No","Sí"),labels = c("No","Si"))

#Pasamos la variable "Sexo" suponiendo que las mujeres leen mas
datos$Sexo <- factor(datos$Sexo ,levels = c("Masculino","Femenino"), labels = c("Masculino","Femenino"))
table(datos$Sexo)

#Pasamos la variable "Menores" suponiendo que las mujeres leen mas
datos$Menores <- factor(datos$Menores ,levels = c("No","Si"))
table(datos$Menores)


#Tiene medicina prepaga u obra social?
table( datos$Medicina , useNA = "always")
datos$Medicina <- ifelse(is.na(datos$Medicina),"Solo pública",datos$Medicina)
datos$Medicina <- factor(datos$Medicina, levels = c("Solo pública","Obra social o prepaga"),labels = c("No","Si"))
table( datos$Medicina , useNA = "always")

#Ponemos nombres mas cortos a la variable "Educacion" para que el WOE sea mas claro
table(datos$Educacion)
datos$Educacion <- factor(datos$Educacion,levels = c("Educación especial","Ns/Nc","Nunca estudió","Preescolar"
                                                     ,"Primario incompleto","Primario completo","Secundario incompleto",
                                                     "Secundario completo", "Terciario o superior incompleto","Terciario completo o más"),
                      labels = c("EE","Ns/Nc","No","Pre","Primario I","Primario","Secundario I","Secundario","Superior I","Superior"))

#Sacamos los NA de Ocupación y dejamos igual lo otro para pasar por el WOE
datos$Ocupacion <- ifelse(is.na(datos$Ocupacion),"No trabaja",datos$Ocupacion)


#Valor de la información: calcula un ratio de la proporción de "buenos" y "malos" para cada categoria
#de la variable donde un valor de:
#Menor de 0.02     = no es util
#Entre 0.02 y 0.10 = debil
#Entre 0.10 y 0.30 = medio
#Mayor de 0.30     = util
s <- scorecard::iv(datos[,c(1:7,9:10)], y = "Lee", positive = "Si")
#Al grafico se le agregan los valores de corte
barplot(s$info_value,ylim=c(0,0.35),names.arg = s$variable, col="deepskyblue",main="Valor IV de cada variable")
abline(h=c(0.02,0.1,0.3),lwd=2, lty=2,col="red")

################################
#### Colapso de categorías  ####
################################

#Para esto utilizamos la función WOE = Weights of Evidence, esta función genera grupos óptimos de variables
WOE <- woebin(datos, y="Lee", var_skip = "Ponderacion" ,positive = "Si" ,bin_num_limit = 10)

#Vemos los graficos de cada variable que son mas claros de modo de colapsar categorias
woebin_plot(WOE)

#Vemos que para educación creo 3 categorías y rearmamos la variable como:
#Educacion Baja  = Educacion Especial, Ns/Nc, No estudio, Preescolar, Primario Incompleto
#Educacion Media = Primario, Secundario Incompleto, Secundario
#Educacion Alta  = Superior Incompleto y Completo
table(datos$Educacion , useNA = "always")
levels(datos$Educacion)[levels(datos$Educacion) %in% c("EE","Ns/Nc","No","Pre","Primario I")] <- "Bajo"
levels(datos$Educacion)[levels(datos$Educacion) %in% c("Primario","Secundario I","Secundario")] <- "Medio"
levels(datos$Educacion)[levels(datos$Educacion) %in% c("Superior I","Superior")] <- "Alto"
table(datos$Educacion)

#Vamos ahora por la variable ocupación
table(datos$Ocupacion , useNA = "always")
WOE$Ocupacion

#Vamos a usar un criterio diferente al dado por el WOE
#Sin ingresos   = 99, Ama de Casa, Estudiante, Inactivo, No trabaja, ns/nr, sin salario
#Bajos ingresos = Cuenta propia/changas, Domestico, Jubilado, Pasante, Programa
#Con ingresos   = Privado, Publico, Patron
#La pasamos a factor primero
datos$Ocupacion <- factor(datos$Ocupacion)

#Asignamos los niveles
levels(datos$Ocupacion)[levels(datos$Ocupacion) 
              %in% c("99","Ama de casa","Estudiante","Inactivo (por ej rentista)","No trabaja","NS / NR",
                     "NS/NR","Trabajador sin salario")]                        <- "Sin ingresos"
levels(datos$Ocupacion)[levels(datos$Ocupacion) 
              %in% c("Cuenta propia/ changas","Empleado del servicio doméstico","Jubilado /pensionado",
                     "Pasante / aprendiz /becario","Trabajador en un programa de empleo")] <- "Bajos ingresos"
levels(datos$Ocupacion)[levels(datos$Ocupacion)
              %in% c("Empleado u obrero en una empresa privada","Empleado u obrero en una institución pública",
                     "Patrón o empleador(emplea personal)")] <- "Ingresos"

#Verificamos la tabla final
table(datos$Ocupacion , useNA = "always")

#Realizamos nuevamente el analisis del WOE
WOE <- woebin(datos, y="Lee", var_skip = "Ponderacion" ,positive = "Si" ,bin_num_limit = 10)

#Vemos los gráficos de cada variable
woebin_plot(WOE)

##################################
####  Regresión de los datos  ####
##################################

####Regresion y Resultados####
#Hacemos la regresión logística a traves de "glm" y debemos definir que tipo de regresion (family = binomial), "link" no es necesario
#Los comentarios no hace nada
logit<-glm(Lee ~ Ingreso + Miembros + Edad + Sexo + Medicina + Educacion + Ocupacion + Menores 
           ,data = datos , family = binomial(link = "logit"))


# 1 - ¿Como se interpretan los coeficientes?

#Pedimos los coeficientes, la prueba es una Z ya que es similar al Wald
round(summary(logit)$coefficients,5)

#Para ver la relacion entre z y wald
#https://stats.stackexchange.com/questions/60074/wald-test-for-logistic-regression
#Construyamos la tabla de coeficientes con el valor del WALD (la z al cuadrado) y su PV oara una chi con un GL
#Guardamos los coeficientes con 4 decimales
{d<-round(summary(logit)$coefficients,5)
  
#Le pegamos a los coeficientes el WALD y los PV
e<-data.frame(d,Wald=(d[,1]/d[,2])^2,"P-Value"=pchisq((d[,1]/d[,2])^2,1,lower.tail=FALSE),check.names = F)
round(e,5)}

##################################
#### Intervalos de confianza  ####
####   de las estimaciones    ####
##################################

# 2 - Que valor ponemos a prueba en cada intervalo para probar la significatividad?

#Vemos los intervalos confianza al 95% de los estimadores, usamos "confint.default" ya que se basa en la normalidad asintotica
cbind(coef(logit),confint.default(logit))

#Vemos los odds-ratio y sus IC al 95%
exp(cbind(coef(logit),confint.default(logit)))

################################
####        Estimación      ####
####  de la matriz var-cov  ####
################################

#https://stats.stackexchange.com/questions/167324/variance-covariance-matrix-of-logit-with-matrix-computation

#Para esto hacemos una nueva regresión como ejemplo tomando de las cuali las dicotomicas
#Si no, la matriz de datos es diferente por las categoricas que tienen varias dummy
logitB<-glm(Lee ~ Ingreso + Miembros + Edad + Sexo + Medicina
           ,data = datos , family = binomial(link = "logit"))

#Armamos una matriz de las independientes con el 1 adelante para la constante
x <- model.matrix(logitB)

#Tomamos los estimados de la regresión
est <- logitB$fit

#Aramos la matriz W (Weights) como pi * (1 - pi)
w <- est * (1 - est)

#Y armamos la matriz diagonal V poniendo los W en la diagonal
v <-diag(w)

#Con esto podemos calcular la inversa de XT * W * X
var_b<-solve(t(x) %*% v %*% x)
var_b

#Sacamos la misma matriz con una funcion y vemos que son igual
vcov(logitB)

################################
####   Estimación robusta   ####
####  de la matriz var-cov  ####
################################

#https://stats.stackexchange.com/questions/283801/how-are-robust-standard-errors-calculated-in-the-case-of-logistic-regression

#el argumento "working", ver ?glm > value > residuals
r <- residuals(logitB, "working") * sqrt(weights(logitB, "working"))
X <- model.matrix(logitB) * sqrt(weights(logitB, "working"))
solve(t(X) %*% X) %*% t(X) %*% diag(r^2) %*% X %*% solve(t(X) %*% X)
sandwich(logitB)

#Es la misma expresion usando las var-cov originales
var_b %*% t(X) %*% diag(r^2) %*% X  %*%var_b

coeftest(logit)
coeftest(logit,  vcov = vcovHC, type = "HC1")
sqrt(diag(vcovHC(logit)))

#############################
####  Medidas de Bondad  ####
####      de ajuste      ####
#############################

#Vemos ahora los -2LL al introducir cada variable, primero la constante, luego sexo, ingreso, estudio y trabaja.
#Los GL son por las categorías que tiene la variable
anova(logit)

#Pedimos las medidas de bondad de ajuste,especificamos McFadden, Cox y Nalgerkerke
PseudoR2(logit,c("McFadden","CoxSnell","Nagelkerke"))

#Mas completo en cuanto a medidas
PseudoR2(logit , which="all")

#Pedimos el estadistico Hosmer-Lemeshow, especificamos la variable target (variable dependiente) 
# y el esperado con "fitted(c)", g=grupos
f<-logitgof(datos$Lee,fitted(logit),g=10)
f

#Pedimos la tabla de Hosmer-Lemeshow, la prueba genera los observados y los esperados
g<-data.frame(cbind(f$observed,f$expected))
colnames(g)<-c("Obs=0","Obs=1","Pred=0","Pred=1")
g

#########################
####  Tabla de       ####
####  clasificación  ####
#########################

#Generamos las predicciones, "type=response" hace que la predicción sea para una logística
prediccion<-predict(logit, type="response")

#Generamos la pertenencia en funcion al punto de corte dado por "corte" y debemos definir cual valor de la variable target es el 1 con positive
#https://topepo.github.io/caret/measuring-performance.html

corte<-0.9
i<-ifelse(prediccion>corte,1,0)

#Pasamos a factor ambas variables, la predicción y la observación
i<-factor(i,levels=c(0:1),labels = c("No","Si"))

#Tabla de clasificación
clasif<-confusionMatrix(data = i, reference = datos$Lee , positive = "Si");clasif
#Siendo en la tabla "Si" el target o presencia
#                A  B
#                C  D
#Sensibilidad = D / B+D
#Especificidad = A/A+C
#Pos Pred Value = D/C+D
#Neg Pred Value = A/A+B
#Prevalencia = B+D/N = 79/150
#Detection rate D/n =aciertos/n = 70/150
#Detection Prevalence = C+D/n = 95/150
#Balance Accuracy = (Sensibilidad+Especificidad)/2=(0.886+.648)/2

#Vemos la tabla como porcentaje, la proporción de aciertos esta dada por la diagonal principal (Sensibilidad=1, Especificidad=0)
prop.table(clasif$table,margin=2)

#Pedimos los marginales fila y Proporción de Positivos y de Negativos
prop.table(clasif$table,margin=1)

#Generamos las tablas para diversos puntos de corte, en este 10 entre 0.1 y 1
for (i in seq(0.31,0.4,length=10)) {
  r<-ifelse(prediccion>i,1,0)
  r<-factor(r,levels=c(0:1),labels = c("No","Si"))
  s<-confusionMatrix(r, datos$Lee , positive = "Si")
  print(i)
  print(s)
}

#####################
####  Curva ROC  ####
#####################

#Esta función permite calcular los valores necesarios para la curva ROC, el corte, la sensibilidad y la especificidad
curva<-roc(datos$Lee,prediccion)

#Gráfico mas potable
plot(1-curva$specificities,curva$sensitivities,type="l",xlab="1-Especificidad",ylab="Sensibilidad",lwd=2,col="blue",xaxs="i",yaxs="i")
segments(0,0,1,1,lwd=2,col="red")

#Vemos los puntos de corte, la Sensibilidad y 1-Especificidad
tabla<-round(data.frame(Corte=curva$thresholds,Sensibilidad=curva$sensitivities,`1-Especificidad`=1-curva$specificities),4);tabla

#Calculamos el area bajo la curva
(areaRoc<-auc(datos$Lee , prediccion))

#Calculamos los IC del area bajo la curva
ci(areaRoc)

#Gráfico de las probabilidades predichas para cada grupo, me copie de Jezabel, para que no se pisen los colores "scales::alpha('grey',.6)"
hist(prediccion[datos$Lee=="No"], xlim=c(0,1), ylim=c(0,600), breaks = 40, freq=T, col='skyblue2',border=F, xlab="Probabilidad de participar", main="Grafico de Probabilidades",xaxp=c(0,1,20),cex.axis=.8)
hist(prediccion[datos$Lee=="Si"], xlim=c(0,1), ylim=c(0,600), breaks = 40, freq=T, add=T,col=scales::alpha('grey50',.6),border=F)
legend(x="topleft", fill=c('skyblue2',scales::alpha('grey50',.6)), legend=c("No Conoce", "Conoce"),bty = 'n',border = NA)

#Grafico de densidades altura = breaks * f1 / n
hist(prediccion[datos$Lee=="No"], xlim=c(0,1), ylim=c(0,10), breaks = 40, freq=F, col='skyblue2',border=F, xlab="Probabilidad de participar", main="Grafico de Probabilidades",xaxp=c(0,1,20),cex.axis=.8)
hist(prediccion[datos$Lee=="Si"], xlim=c(0,1), ylim=c(0,10), breaks = 40, freq=F, add=T,col=scales::alpha('grey50',.6),border=F,xaxp=c(0,1,20),cex.axis=.8)
legend(x="topleft", fill=c('skyblue2',scales::alpha('grey50',.6)), legend=c("No Conoce", "Conoce"),bty = 'n',border = NA)
lines(density(prediccion[datos$Lee=="No"]), col="blue2",lwd=2)
lines(density(prediccion[datos$Lee=="Si"]), col=scales::alpha('grey20',.6),lwd=2)

#Ver de usar el performance del paquete ROCR, debemos usar las predicciones
#Esta función usa las predicciones y los valores de la variable respuesta
pred<-prediction(prediccion,datos$Lee)

#Con la función anterior, guardamos los tpr (verdaderos positivos) y los fpr (falsos positivos)
p<-performance(pred,"tpr","fpr")

###############################
#### Punto de Corte por KS ####
###############################

#El KS viene de Kolmogorov-Smirnov y busca la maxima diferencia entre sensibilidad y Especificidad

#Buscamos el maximo de la distancia entre verdaderos y falsos positivos el KS
ks<- max(p@y.values[[1]]-p@x.values[[1]]);ks

#Hacemos el gráfico buscando el KS
plot(p@x.values[[1]],type="l",lwd=2,col="red",ylab="Cortes")
par(new=T)
plot(p@y.values[[1]],type="l",lwd=2,col="green4",ylab="Cortes",xaxt="n")
f<-which.max(p@y.values[[1]]-p@x.values[[1]])
segments(f,p@x.values[[1]][f],f,p@y.values[[1]][f],lwd = 2,col="red")

#Grafico CUrva ROC a partir de performance
plot(p,col="darkblue",lwd=2,xaxs="i",yaxs="i")
box()
abline(0,1, lty = 300, col = "green4")
grid(col="aquamarine")


##############################
####  Punto de Corte por  ####
####  criterio de Youden  ####
##############################

#El criterio de Youden fija el punto de corte en el maximo de Especificidad + Sensibilidad

#Buscamos el optimo segun el criterio de Youden, es el max(esp+sens) de los cortes
#Vemos los valores para el optimo con el argumento "best"
#Vemos todos con "all"
opt<-coords(curva, "best", ret=c("threshold", "specificity", "sensitivity", "accuracy",
                                  "tn", "tp", "fn", "fp", "npv", "ppv", "1-specificity",
                                  "1-sensitivity", "1-accuracy", "1-npv", "1-ppv",
                                  "precision", "recall"))

#Grafico mas potable
#plot(1-curva$specificities,curva$sensitivities,type="l",xlab="1-Especificidad",ylab="Sensibilidad",lwd=2,col="blue",main="Curva ROC")
#segments(0,0,1,1,lwd=2,col="red")
#points(1-opt[[2]],opt[[3]],col="red",pch=19)

#Coordenada del corte de Youden, el primer valor es la Especificidad (ojo)
#legend(1-opt[[2]],.15+opt[[3]],paste("(",round(1-opt[[2]],4),",",round(opt[[3]],4),")"),col="red",bty="n")

plot(curva$thresholds, curva$sensitivities, type="l",lwd=2,col="red",xlab="Corte")
lines(curva$thresholds,curva$specificities, lwd=2, col="green4")
abline(v=opt$threshold,lty=2,lwd=2)
legend("bottom",lwd=2,legend = c("Sensibilidad","Especificidad"),bty="n",col=c("red","green4"))

##############################
#### Criterio de Accuracy ####
##############################

#Se define por el punto donde se corta la curva de sensibilidad, de especificidad y el accuracy
#https://www.kaggle.com/code/untrefhugodelfino/telco-customer-churn-logisticregression-untref/edit

#Definimos 100 puntos de corte para evaluar en cada punto las tres medidas
z <- seq(0.01,1,length.out=100)

#Creamos el elemento donde guardar los resultados
d <- data.frame(matrix(0))

#Para cada punto evaluamos
for(i in 1 :100){
  c<-ifelse(prediccion>z[i],1,0)
  c<-factor(c,levels=c(0:1),labels = c("No","Si"))
  clasif<-confusionMatrix(data = c, reference = datos$Lee , positive = "Si")
  d[i,1] <- z[i]
  d[i,2]  <- clasif$byClass[1]
  d[i,3] <- clasif$byClass[2]
  d[i,4]      <- clasif$overall[1]
  
}

colnames(d) <- c("Corte","Sensibilidad","Especificidad","Accuracy")

#El Corte lo buscamos como la minima diferencia
cut <- d$Corte[which.min(abs(d$Sensibilidad-d$Especificidad))]

#Graficamos
plot(z,d$Sensibilidad,type="l",lwd=2,col="red",xlab="Corte")
lines(z,d$Especificidad,lwd=2,col="green4")
lines(z,d$Accuracy,lwd=2,col="deepskyblue")
legend("bottom",lwd=2,legend = c("Sensibilidad","Especificidad","Accuracy"),bty="n",
       col=c("red","green4","deepskyblue"))
abline(v=cut,lwd=2,lty=2)
axis(1,cut,labels = cut)

################################
####  Regresión de a pasos  ####
################################

#La primera regresión introdujo todas las variables sin tener en cuenta la significatividad, el "stepwise" permite introducir de paso a paso
#y de a una las variables
p<-stepwise(logit, direction='forward', criterion='AIC')
summary(p)

#Con esto vemos la información en cada uno de los pasos
p$anova

#Hacemos la nueva regresión sin "Edad", "Ocupacion" y "Menores"
logit2<-glm(Lee ~ Ingreso + Miembros + Medicina + Educacion + Sexo  
            ,data = datos , family = binomial(link = "logit"))

#Vemos el resultado
summary(logit2)

#Los Pseudo R2
PseudoR2(logit2,which = "all")

#####################################
#### Seleccion del mejor subset  ####
#####################################

library("bestglm")
#Las funciones de este paquete requieren del data.frame de los datos 
#y que ese armado de la forma XY (las X primero y luego la Y)

#Armamos un dataset solo con cuantis y dicotomicas
base <- cbind(datos[,1:5],datos[,10],datos[,7])
colnames(base)[6:7] <-c("Menores","Lee")


g <- bestglm( base,               #datos a usar
              family = binomial,       #familia de la regresión (regresion es Gauss)
              IC="CV",                #Criterio de información para analizar
              qLevel = 0.95,           #Alfa para determinar la decisión
              TopModels = 5  )        #Los mejores 5 modelos

g
summary(g)

#########################
#### Modelo Probit   ####
#########################

#Tomamos el modelo optimo para la estimación Probit
#La unica diferencia es que el "link" debe ser probit
probit <- glm(Lee ~ Ingreso + Miembros + Medicina + Educacion + Ocupacion  
            ,data = datos , family = binomial(link = "probit"))


#Vemos los coeficientes
summary(probit)


#Para ver la diferencia entre probit y logit
#Use of the Probit Link Function in Generalized Linear Models.pdf pag 166
a <- probit$residuals
ks.test(a,"pnorm",mean(a),sd(a))
hist(a,breaks = 40 , col="deepskyblue")

logit4 <- glm(Lee ~ Ingreso + Miembros + Medicina + Educacion + Ocupacion  
              ,data = datos , family = binomial(link = "logit") , weights = 1/Ponderacion)

#################################
#### Comparación de modelos  ####
#################################

# ¿Que vemos en la comparación de ambos modelos
stargazer::stargazer(logit,logit2, probit, logit4 , type = "text")

#Generamos las predicciones, "type=response" hace que la prediccion sea para una logistica
pred_probit<-predict(probit, type="response")

#Esta funcion permite calcular los valores necesarios para la curva ROC, el corte, la sensibilidad y la especificidad
curva_p<-roc(datos$Lee , pred_probit);curva_p

#Gráfico mas potable
plot(1-curva2$specificities,curva2$sensitivities,type="l",xlab="1-Especificidad",ylab="Sensibilidad",lwd=2,col="blue",xaxs="i",yaxs="i")
segments(0,0,1,1,lwd=2,col="red")
par(new=T)
plot(1-curva_p$specificities,curva_p$sensitivities,type="l",xlab="1-Especificidad",ylab="Sensibilidad",lwd=2,col="green4",xaxs="i",yaxs="i")
#No vemos diferencias en las curvas ROC de ambos modelos

##########################
#### Estimación del   ####
####   mejor subset   ####
##########################

#Estima buscando las mejores variables
logit3   <-
  glmulti::glmulti(Lee ~ Ingreso + Miembros + Edad + Sexo + Medicina + Educacion + Ocupacion + Menores , data = datos,
          level = 1,               # No hay interacciones
          method = "h",            # Analisis exhaustivo
          crit = "aic",            # AIC como criterio
          confsetsize = 5,         # Se queda con los mejores 5 modelos
          plotty = T, report = T,  # Sin dibujo ni reporte
          fitfunction = "glm",     # glm function
          family = binomial(link = "logit"))       # Familia de la binomial

summary(logit3@objects[[1]])

plot(logit3, type = "p")

